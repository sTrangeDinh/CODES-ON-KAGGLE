{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "757a1209",
   "metadata": {
    "papermill": {
     "duration": 0.00507,
     "end_time": "2026-01-24T07:03:48.691355",
     "exception": false,
     "start_time": "2026-01-24T07:03:48.686285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. IMPORT LIBARIES + PATHS + REPRODUCBILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3931e89d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T07:03:48.700362Z",
     "iopub.status.busy": "2026-01-24T07:03:48.700063Z",
     "iopub.status.idle": "2026-01-24T07:04:01.156661Z",
     "shell.execute_reply": "2026-01-24T07:04:01.155900Z"
    },
    "papermill": {
     "duration": 12.463158,
     "end_time": "2026-01-24T07:04:01.158285",
     "exception": false,
     "start_time": "2026-01-24T07:03:48.695127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DATA_DIR: /kaggle/input/csiro-biomass\n",
      "✅ WEIGHTS_DIR: /kaggle/input/pretrained-pytorch-models\n",
      "✅ DEVICE: cuda\n",
      "✅ IMG_SIZE: 224\n",
      "✅ Train CSV: /kaggle/input/csiro-biomass/train.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 1 — Imports + Paths + Reproducibility\n",
    "# ============================================\n",
    "\"\"\"\n",
    "Kaggle to GitHub Sync Configuration\n",
    "Repository: CODES-ON-KAGGLE\n",
    "\"\"\"\n",
    "# Import\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ✅ Competition directory (as you stated)\n",
    "DATA_DIR = \"/kaggle/input/csiro-biomass\"\n",
    "\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\n",
    "TEST_CSV  = os.path.join(DATA_DIR, \"test.csv\")\n",
    "# SAMPLE_SUB_CSV = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "# ✅ Pretrained weights dataset input for model resnet18\n",
    "WEIGHTS_DIR = \"/kaggle/input/pretrained-pytorch-models\"\n",
    "\n",
    "# Inference\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ✅ NotebookA image size (introduced once because Notebook A had no size variable)\n",
    "IMG_SIZE = 224  # used everywhere for resize-only transforms (no pad, no crop)\n",
    "\n",
    "# Safety checks (fail fast if something isn't mounted)\n",
    "for p in [DATA_DIR, WEIGHTS_DIR, TRAIN_CSV, TEST_CSV]:\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Missing expected path: {p}\")\n",
    "\n",
    "print(\"✅ DATA_DIR:\", DATA_DIR)\n",
    "print(\"✅ WEIGHTS_DIR:\", WEIGHTS_DIR)\n",
    "print(\"✅ DEVICE:\", DEVICE)\n",
    "print(\"✅ IMG_SIZE:\", IMG_SIZE)\n",
    "print(\"✅ Train CSV:\", TRAIN_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd364788",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T07:04:01.167609Z",
     "iopub.status.busy": "2026-01-24T07:04:01.166885Z",
     "iopub.status.idle": "2026-01-24T07:04:01.174658Z",
     "shell.execute_reply": "2026-01-24T07:04:01.173886Z"
    },
    "papermill": {
     "duration": 0.013604,
     "end_time": "2026-01-24T07:04:01.175910",
     "exception": false,
     "start_time": "2026-01-24T07:04:01.162306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SquarePad + build_letterbox_resize_transform ready.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 1.5  Optimized SquarePad (Letterbox) + Resize\n",
    "# ============================================\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "class SquarePad:\n",
    "    \"\"\"\n",
    "    Letterbox to square by symmetric padding (constant fill).\n",
    "    Works with PIL.Image inputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, fill=0, padding_mode=\"constant\"):\n",
    "        self.fill = fill\n",
    "        self.padding_mode = padding_mode\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # PIL.Image: size = (W, H)\n",
    "        w, h = img.size\n",
    "        if w == h:\n",
    "            return img  # fast path\n",
    "\n",
    "        max_wh = w if w > h else h\n",
    "        pad_left = (max_wh - w) // 2\n",
    "        pad_top  = (max_wh - h) // 2\n",
    "        pad_right  = max_wh - w - pad_left\n",
    "        pad_bottom = max_wh - h - pad_top\n",
    "\n",
    "        # torchvision expects padding as (left, top, right, bottom)\n",
    "        return F.pad(\n",
    "            img,\n",
    "            (pad_left, pad_top, pad_right, pad_bottom),\n",
    "            fill=self.fill,\n",
    "            padding_mode=self.padding_mode,\n",
    "        )\n",
    "\n",
    "def build_letterbox_resize_transform(\n",
    "    img_size,\n",
    "    mean=(0.485, 0.456, 0.406),\n",
    "    std=(0.229, 0.224, 0.225),\n",
    "    fill=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    SquarePad(letterbox) -> Resize -> ToTensor -> Normalize\n",
    "    \"\"\"\n",
    "    # Robust interpolation handling across torchvision versions\n",
    "    try:\n",
    "        interp = transforms.InterpolationMode.BILINEAR\n",
    "    except AttributeError:\n",
    "        interp = Image.BILINEAR\n",
    "\n",
    "    return transforms.Compose([\n",
    "        SquarePad(fill=fill, padding_mode=\"constant\"),\n",
    "        transforms.Resize((img_size, img_size), interpolation=interp),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ])\n",
    "\n",
    "print(\"✅ SquarePad + build_letterbox_resize_transform ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eba87b",
   "metadata": {
    "papermill": {
     "duration": 0.00366,
     "end_time": "2026-01-24T07:04:01.183288",
     "exception": false,
     "start_time": "2026-01-24T07:04:01.179628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. SETUP WEIGHTS FOR THE OFFLINE RESNET 18\n",
    "Since we run code when the Internet is off, we have to manually input a dataset on Kaggle\n",
    "that contains ResNet18 ImageNet weights.\n",
    "The ResNet18 ImageNet weights will be named resnet18-5c106cde.pth or resnet18.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46d032f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T07:04:01.191793Z",
     "iopub.status.busy": "2026-01-24T07:04:01.191476Z",
     "iopub.status.idle": "2026-01-24T07:04:02.655646Z",
     "shell.execute_reply": "2026-01-24T07:04:02.654802Z"
    },
    "papermill": {
     "duration": 1.470335,
     "end_time": "2026-01-24T07:04:02.657248",
     "exception": false,
     "start_time": "2026-01-24T07:04:01.186913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] weights_dir: /kaggle/input/pretrained-pytorch-models\n",
      "[DEBUG] resnet18 candidates found: 1\n",
      "[DEBUG] first few candidates: ['/kaggle/input/pretrained-pytorch-models/resnet18-5c106cde.pth']\n",
      "[DEBUG] RESNET18_WEIGHTS selected: /kaggle/input/pretrained-pytorch-models/resnet18-5c106cde.pth\n",
      "[DEBUG] Feature extractor ready.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 2a — Model / Weights Setup (ResNet18 offline) + Robust Loader + Debug Prints\n",
    "# (UPDATED: uses optimized SquarePad(letterbox)+Resize transform builder)\n",
    "# ============================================\n",
    "def find_resnet18_weights(weights_dir: str) -> str:\n",
    "    print(\"[DEBUG] weights_dir:\", weights_dir)\n",
    "\n",
    "    wd = Path(weights_dir)\n",
    "    if not wd.exists():\n",
    "        raise FileNotFoundError(f\"Missing weights dataset. Expected directory: {weights_dir}\")\n",
    "\n",
    "    candidates = []\n",
    "    for p in wd.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in {\".pth\", \".pt\"} and \"resnet18\" in p.name.lower():\n",
    "            candidates.append(str(p))\n",
    "\n",
    "    print(\"[DEBUG] resnet18 candidates found:\", len(candidates))\n",
    "    if candidates:\n",
    "        print(\"[DEBUG] first few candidates:\", candidates[:5])\n",
    "\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"No resnet18 .pth/.pt file found under {weights_dir}\")\n",
    "\n",
    "    candidates_sorted = sorted(\n",
    "        candidates,\n",
    "        key=lambda x: (0 if re.search(r\"resnet18-.*\\.pth$\", os.path.basename(x).lower()) else 1, len(x))\n",
    "    )\n",
    "    chosen = candidates_sorted[0]\n",
    "    print(\"[DEBUG] RESNET18_WEIGHTS selected:\", chosen)\n",
    "    return chosen\n",
    "\n",
    "RESNET18_WEIGHTS = find_resnet18_weights(WEIGHTS_DIR)\n",
    "\n",
    "def _to_state_dict(obj):\n",
    "    \"\"\"Convert various checkpoint formats into a state_dict dict.\"\"\"\n",
    "    if isinstance(obj, nn.Module):\n",
    "        return obj.state_dict()\n",
    "    if isinstance(obj, dict):\n",
    "        if \"state_dict\" in obj and isinstance(obj[\"state_dict\"], dict):\n",
    "            return obj[\"state_dict\"]\n",
    "        if all(isinstance(k, str) for k in obj.keys()):\n",
    "            return obj\n",
    "    raise TypeError(f\"Unsupported checkpoint type: {type(obj)}\")\n",
    "\n",
    "def build_resnet18_feature_extractor(weights_path: str, device: str):\n",
    "    model = models.resnet18(weights=None)  # no internet download\n",
    "    raw = torch.load(weights_path, map_location=\"cpu\", weights_only=False)\n",
    "    state = _to_state_dict(raw)\n",
    "\n",
    "    cleaned = {}\n",
    "    for k, v in state.items():\n",
    "        k2 = k\n",
    "        for prefix in (\"module.\", \"model.\"):\n",
    "            if k2.startswith(prefix):\n",
    "                k2 = k2[len(prefix):]\n",
    "        cleaned[k2] = v\n",
    "\n",
    "    missing, unexpected = model.load_state_dict(cleaned, strict=False)\n",
    "    if missing:\n",
    "        print(\"Warning: missing keys when loading weights:\", missing[:5], (\"...\" if len(missing) > 5 else \"\"))\n",
    "    if unexpected:\n",
    "        print(\"Warning: unexpected keys when loading weights:\", unexpected[:5], (\"...\" if len(unexpected) > 5 else \"\"))\n",
    "\n",
    "    extractor = nn.Sequential(*list(model.children())[:-1]).to(device)\n",
    "    extractor.eval()\n",
    "    for p in extractor.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # ✅ Letterboxing enabled (optimized): SquarePad -> Resize -> ToTensor -> Normalize\n",
    "    tfm = build_letterbox_resize_transform(\n",
    "        IMG_SIZE,\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225),\n",
    "        fill=0,\n",
    "    )\n",
    "    return extractor, tfm\n",
    "\n",
    "EXTRACTOR, IMG_TFM = build_resnet18_feature_extractor(RESNET18_WEIGHTS, DEVICE)\n",
    "print(\"[DEBUG] Feature extractor ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdab73ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T07:04:02.666786Z",
     "iopub.status.busy": "2026-01-24T07:04:02.666458Z",
     "iopub.status.idle": "2026-01-24T07:04:02.943017Z",
     "shell.execute_reply": "2026-01-24T07:04:02.942196Z"
    },
    "papermill": {
     "duration": 0.283113,
     "end_time": "2026-01-24T07:04:02.944474",
     "exception": false,
     "start_time": "2026-01-24T07:04:02.661361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] weights_dir: /kaggle/input/pretrained-pytorch-models\n",
      "[DEBUG] resnet18 candidates found: 1\n",
      "[DEBUG] first few candidates: ['/kaggle/input/pretrained-pytorch-models/resnet18-5c106cde.pth']\n",
      "[DEBUG] RESNET18_WEIGHTS selected: /kaggle/input/pretrained-pytorch-models/resnet18-5c106cde.pth\n",
      "[DEBUG] Feature extractor ready.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 2b — Model / Weights Setup (ResNet18 offline) + Robust Loader + Debug Prints\n",
    "# (Duplicated in Notebook A — UPDATED: uses optimized SquarePad(letterbox)+Resize transform builder)\n",
    "# ============================================\n",
    "def find_resnet18_weights(weights_dir: str) -> str:\n",
    "    print(\"[DEBUG] weights_dir:\", weights_dir)\n",
    "\n",
    "    wd = Path(weights_dir)\n",
    "    if not wd.exists():\n",
    "        raise FileNotFoundError(f\"Missing weights dataset. Expected directory: {weights_dir}\")\n",
    "\n",
    "    candidates = []\n",
    "    for p in wd.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in {\".pth\", \".pt\"} and \"resnet18\" in p.name.lower():\n",
    "            candidates.append(str(p))\n",
    "\n",
    "    print(\"[DEBUG] resnet18 candidates found:\", len(candidates))\n",
    "    if candidates:\n",
    "        print(\"[DEBUG] first few candidates:\", candidates[:5])\n",
    "\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"No resnet18 .pth/.pt file found under {weights_dir}\")\n",
    "\n",
    "    candidates_sorted = sorted(\n",
    "        candidates,\n",
    "        key=lambda x: (0 if re.search(r\"resnet18-.*\\.pth$\", os.path.basename(x).lower()) else 1, len(x))\n",
    "    )\n",
    "    chosen = candidates_sorted[0]\n",
    "    print(\"[DEBUG] RESNET18_WEIGHTS selected:\", chosen)\n",
    "    return chosen\n",
    "\n",
    "RESNET18_WEIGHTS = find_resnet18_weights(WEIGHTS_DIR)\n",
    "\n",
    "def _to_state_dict(obj):\n",
    "    \"\"\"Convert various checkpoint formats into a state_dict dict.\"\"\"\n",
    "    if isinstance(obj, nn.Module):\n",
    "        return obj.state_dict()\n",
    "    if isinstance(obj, dict):\n",
    "        if \"state_dict\" in obj and isinstance(obj[\"state_dict\"], dict):\n",
    "            return obj[\"state_dict\"]\n",
    "        if all(isinstance(k, str) for k in obj.keys()):\n",
    "            return obj\n",
    "    raise TypeError(f\"Unsupported checkpoint type: {type(obj)}\")\n",
    "\n",
    "def build_resnet18_feature_extractor(weights_path: str, device: str):\n",
    "    model = models.resnet18(weights=None)  # no internet download\n",
    "    raw = torch.load(weights_path, map_location=\"cpu\", weights_only=False)\n",
    "    state = _to_state_dict(raw)\n",
    "\n",
    "    cleaned = {}\n",
    "    for k, v in state.items():\n",
    "        k2 = k\n",
    "        for prefix in (\"module.\", \"model.\"):\n",
    "            if k2.startswith(prefix):\n",
    "                k2 = k2[len(prefix):]\n",
    "        cleaned[k2] = v\n",
    "\n",
    "    missing, unexpected = model.load_state_dict(cleaned, strict=False)\n",
    "    if missing:\n",
    "        print(\"Warning: missing keys when loading weights:\", missing[:5], (\"...\" if len(missing) > 5 else \"\"))\n",
    "    if unexpected:\n",
    "        print(\"Warning: unexpected keys when loading weights:\", unexpected[:5], (\"...\" if len(unexpected) > 5 else \"\"))\n",
    "\n",
    "    extractor = nn.Sequential(*list(model.children())[:-1]).to(device)\n",
    "    extractor.eval()\n",
    "    for p in extractor.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # ✅ Letterboxing enabled (optimized): SquarePad -> Resize -> ToTensor -> Normalize\n",
    "    tfm = build_letterbox_resize_transform(\n",
    "        IMG_SIZE,\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225),\n",
    "        fill=0,\n",
    "    )\n",
    "    return extractor, tfm\n",
    "\n",
    "EXTRACTOR, IMG_TFM = build_resnet18_feature_extractor(RESNET18_WEIGHTS, DEVICE)\n",
    "print(\"[DEBUG] Feature extractor ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4b618b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T07:04:02.953968Z",
     "iopub.status.busy": "2026-01-24T07:04:02.953489Z",
     "iopub.status.idle": "2026-01-24T07:04:02.958565Z",
     "shell.execute_reply": "2026-01-24T07:04:02.957962Z"
    },
    "papermill": {
     "duration": 0.011344,
     "end_time": "2026-01-24T07:04:02.959964",
     "exception": false,
     "start_time": "2026-01-24T07:04:02.948620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ get_tta_transforms() updated to SquarePad(letterbox)+Resize pipeline.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 2.5 — AUGMENTATIONS — TTA Views (validation/inference-only)\n",
    "# (UPDATED: uses optimized SquarePad(letterbox)+Resize transform builder)\n",
    "# ============================================\n",
    "def get_tta_transforms():\n",
    "    \"\"\"\n",
    "    Returns a LIST of torchvision transform pipelines for TTA.\n",
    "    Each pipeline is a different \"view\" of the image.\n",
    "    NOTE: Applied ONLY for validation/inference (not training).\n",
    "    \"\"\"\n",
    "\n",
    "    # Build base (letterbox+resize+tensor+normalize) once, then reuse\n",
    "    base = build_letterbox_resize_transform(\n",
    "        IMG_SIZE,\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225),\n",
    "        fill=0,\n",
    "    )\n",
    "\n",
    "    # View 1: Original\n",
    "    original_view = base\n",
    "\n",
    "    # View 2: Horizontal Flip (always) + base\n",
    "    hflip_view = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=1.0),\n",
    "        *base.transforms,\n",
    "    ])\n",
    "\n",
    "    # View 3: Vertical Flip (always) + base\n",
    "    vflip_view = transforms.Compose([\n",
    "        transforms.RandomVerticalFlip(p=1.0),\n",
    "        *base.transforms,\n",
    "    ])\n",
    "\n",
    "    return [original_view, hflip_view, vflip_view]\n",
    "\n",
    "print(\"✅ get_tta_transforms() updated to SquarePad(letterbox)+Resize pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61940182",
   "metadata": {
    "papermill": {
     "duration": 0.003872,
     "end_time": "2026-01-24T07:04:02.967719",
     "exception": false,
     "start_time": "2026-01-24T07:04:02.963847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2372fa4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T07:04:02.976763Z",
     "iopub.status.busy": "2026-01-24T07:04:02.976533Z",
     "iopub.status.idle": "2026-01-24T07:04:03.047072Z",
     "shell.execute_reply": "2026-01-24T07:04:03.046258Z"
    },
    "papermill": {
     "duration": 0.076768,
     "end_time": "2026-01-24T07:04:03.048428",
     "exception": false,
     "start_time": "2026-01-24T07:04:02.971660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning directory for ground truth: /kaggle/input/csiro-biomass/test\n",
      "✅ Found 1 images in directory.\n",
      "train_df: (1785, 9)\n",
      "train_wide: (357, 7)\n",
      "[DEBUG] train_df columns: ['sample_id', 'image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm', 'target_name', 'target']\n",
      "[DEBUG] train_wide rows : 357\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 3 — Data Loading (train/test)\n",
    "# ============================================\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "TEST_FOLDER_NAME = \"test\"\n",
    "TEST_IMG_DIR = os.path.join(DATA_DIR, TEST_FOLDER_NAME)\n",
    "\n",
    "print(f\"Scanning directory for ground truth: {TEST_IMG_DIR}\")\n",
    "\n",
    "# 3. MANUAL DIRECTORY SCAN\n",
    "# We ignore the provided test.csv and look at the disk\n",
    "try:\n",
    "    # Get all image files sorted alphabetically\n",
    "    all_test_images = sorted([\n",
    "        f for f in os.listdir(TEST_IMG_DIR)\n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "    ])\n",
    "except FileNotFoundError:\n",
    "    print(\"⚠️ WARN: Test directory not found (Local run?). Creating empty DF.\")\n",
    "    all_test_images = []\n",
    "\n",
    "print(f\"✅ Found {len(all_test_images)} images in directory.\")\n",
    "\n",
    "# 4. CONSTRUCT COMPATIBLE DATAFRAME\n",
    "# Trang's code later does: os.path.join(DATA_DIR, p)\n",
    "# So we must ensure 'p' includes the folder name, e.g., \"test/image_01.jpg\"\n",
    "data = []\n",
    "for filename in all_test_images:\n",
    "    relative_path = os.path.join(TEST_FOLDER_NAME, filename)\n",
    "    data.append({\n",
    "        \"image_path\": relative_path,  # This fixes the pathing\n",
    "        \"sample_id\": filename,        # Placeholder\n",
    "        \"target_name\": \"GDM_g\"        # Placeholder (not used for final submission generation)\n",
    "    })\n",
    "\n",
    "test_df = pd.DataFrame(data)\n",
    "\n",
    "print(\"train_df:\", train_df.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# Core Strategy (from LB-0-57-DESCRIPTION): predict 3, reconstruct 5\n",
    "# ===============================================================\n",
    "# 5 targets present in the dataset (used for parsing / completeness checks)\n",
    "ALL_TARGET_NAMES = [\"Dry_Clover_g\", \"Dry_Dead_g\", \"Dry_Green_g\", \"Dry_Total_g\", \"GDM_g\"]\n",
    "\n",
    "# 3 targets to train/predict (exactly as specified in LB-0-57-DESCRIPTION)\n",
    "CORE_TARGET_NAMES = [\"Dry_Total_g\", \"GDM_g\", \"Dry_Green_g\"]\n",
    "\n",
    "# 5 targets to output for submission (order can be any; we keep LB-style ordering)\n",
    "SUBMISSION_TARGET_NAMES = [\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"]\n",
    "\n",
    "def make_train_wide(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      image_path + ALL_TARGET_NAMES columns\n",
    "    Supports:\n",
    "      - modern long format: image_path, target_name, target\n",
    "      - fallback: image_path, target (assumes 5-row blocks per image)\n",
    "    \"\"\"\n",
    "    if {\"image_path\", \"target_name\", \"target\"}.issubset(df.columns):\n",
    "        wide = (\n",
    "            df.pivot_table(index=\"image_path\", columns=\"target_name\", values=\"target\", aggfunc=\"first\")\n",
    "              .reset_index()\n",
    "        )\n",
    "        for t in ALL_TARGET_NAMES:\n",
    "            if t not in wide.columns:\n",
    "                wide[t] = np.nan\n",
    "        return wide[[\"image_path\"] + ALL_TARGET_NAMES]\n",
    "\n",
    "    if {\"image_path\", \"target\"}.issubset(df.columns):\n",
    "        paths = df[\"image_path\"].values\n",
    "        y = df[\"target\"].values\n",
    "        if len(df) % 5 != 0:\n",
    "            raise ValueError(\"Fallback parsing expected train rows multiple of 5.\")\n",
    "        unique_paths = paths[0::5]\n",
    "        wide = pd.DataFrame({\"image_path\": unique_paths})\n",
    "        for i, t in enumerate(ALL_TARGET_NAMES):\n",
    "            wide[t] = y[i::5]\n",
    "        return wide\n",
    "\n",
    "    raise ValueError(\"train.csv must have either (image_path,target_name,target) or (image_path,target).\")\n",
    "\n",
    "train_wide = make_train_wide(train_df)\n",
    "train_wide[\"abs_path\"] = train_wide[\"image_path\"].apply(lambda p: os.path.join(DATA_DIR, p))\n",
    "train_wide = train_wide.dropna(subset=ALL_TARGET_NAMES).reset_index(drop=True)\n",
    "\n",
    "print(\"train_wide:\", train_wide.shape)\n",
    "train_wide.head()\n",
    "\n",
    "# Fail-fast checks (put after train_wide is built)\n",
    "print(\"[DEBUG] train_df columns:\", list(train_df.columns))\n",
    "print(\"[DEBUG] train_wide rows :\", len(train_wide))\n",
    "\n",
    "if len(train_wide) == 0:\n",
    "    raise ValueError(\"train_wide is empty. train.csv parsing likely mismatched the actual format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3660c0d4",
   "metadata": {
    "papermill": {
     "duration": 0.004148,
     "end_time": "2026-01-24T07:04:03.056760",
     "exception": false,
     "start_time": "2026-01-24T07:04:03.052612",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. SET UP A CLASS OF RESNET18 FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00b4a0be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T07:04:03.066557Z",
     "iopub.status.busy": "2026-01-24T07:04:03.065808Z",
     "iopub.status.idle": "2026-01-24T07:04:03.074743Z",
     "shell.execute_reply": "2026-01-24T07:04:03.074241Z"
    },
    "papermill": {
     "duration": 0.015314,
     "end_time": "2026-01-24T07:04:03.076042",
     "exception": false,
     "start_time": "2026-01-24T07:04:03.060728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 4 — Feature Extraction Utilities (stable for Kaggle submission)\n",
    "# ============================================\n",
    "BATCH_SIZE = 2  # keep your setting\n",
    "\n",
    "NUM_WORKERS = 0  # ✅ stable setting for Kaggle submission runs\n",
    "\n",
    "class ImagePathDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = list(image_paths)\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f\"[CELL4] Dataset init: {len(self.image_paths)} images\")\n",
    "        if self.image_paths:\n",
    "            print(\"[CELL4] Example path:\", self.image_paths[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.image_paths[idx]\n",
    "        try:\n",
    "            img = Image.open(p).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            if idx < 3:\n",
    "                print(f\"[CELL4][WARN] Open failed idx={idx}: {p} | {type(e).__name__}\")\n",
    "            img = Image.new(\"RGB\", (IMG_SIZE, IMG_SIZE), (0, 0, 0))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(image_paths, batch_size=BATCH_SIZE, transform=IMG_TFM):\n",
    "    print(f\"[CELL4] extract_features: {len(image_paths)} images | batch_size={batch_size} | workers={NUM_WORKERS}\")\n",
    "\n",
    "    ds = ImagePathDataset(image_paths, transform=transform)\n",
    "    loader = DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,           # ✅ stable\n",
    "        pin_memory=(DEVICE == \"cuda\"),\n",
    "    )\n",
    "\n",
    "    feats = []\n",
    "    total_batches = len(loader)\n",
    "    print(f\"[CELL4] total_batches={total_batches}\")\n",
    "\n",
    "    for b_idx, batch in enumerate(loader):\n",
    "        if b_idx == 0:\n",
    "            print(\"[CELL4] First batch:\", tuple(batch.shape))\n",
    "\n",
    "        batch = batch.to(DEVICE, non_blocking=True)\n",
    "        out = EXTRACTOR(batch)              # [B, 512, 1, 1]\n",
    "        out = out.view(out.size(0), -1)     # [B, 512]\n",
    "        feats.append(out.cpu().numpy())\n",
    "\n",
    "        if (b_idx + 1) == 1 or (b_idx + 1) == total_batches or (b_idx + 1) % max(1, total_batches // 5) == 0:\n",
    "            print(f\"[CELL4] Progress: {b_idx+1}/{total_batches}\")\n",
    "\n",
    "    feats_np = np.vstack(feats) if feats else np.empty((0, 512), dtype=np.float32)\n",
    "    print(\"[CELL4] features shape:\", feats_np.shape)\n",
    "    return feats_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb781a1",
   "metadata": {
    "papermill": {
     "duration": 0.004591,
     "end_time": "2026-01-24T07:04:03.084476",
     "exception": false,
     "start_time": "2026-01-24T07:04:03.079885",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Extract Train Images Features + Make sure the function in Cell 4 runs correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "596bbb1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T07:04:03.093790Z",
     "iopub.status.busy": "2026-01-24T07:04:03.093371Z",
     "iopub.status.idle": "2026-01-24T07:04:37.993412Z",
     "shell.execute_reply": "2026-01-24T07:04:37.992438Z"
    },
    "papermill": {
     "duration": 34.906215,
     "end_time": "2026-01-24T07:04:37.994916",
     "exception": false,
     "start_time": "2026-01-24T07:04:03.088701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL4] extract_features: 357 images | batch_size=16 | workers=0\n",
      "[CELL4] Dataset init: 357 images\n",
      "[CELL4] Example path: /kaggle/input/csiro-biomass/train/ID1011485656.jpg\n",
      "[CELL4] total_batches=23\n",
      "[CELL4] First batch: (16, 3, 224, 224)\n",
      "[CELL4] Progress: 1/23\n",
      "[CELL4] Progress: 4/23\n",
      "[CELL4] Progress: 8/23\n",
      "[CELL4] Progress: 12/23\n",
      "[CELL4] Progress: 16/23\n",
      "[CELL4] Progress: 20/23\n",
      "[CELL4] Progress: 23/23\n",
      "[CELL4] features shape: (357, 512)\n",
      "X_train: (357, 512)\n",
      "Y_train: (357, 3)\n",
      "Core targets: ['Dry_Total_g', 'GDM_g', 'Dry_Green_g']\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 5 — Extract Train Features\n",
    "# ============================================\n",
    "X_train = extract_features(train_wide[\"abs_path\"].tolist(), batch_size=16)\n",
    "\n",
    "# ✅ Core Strategy: train labels are ONLY the 3 core targets\n",
    "Y_train = train_wide[CORE_TARGET_NAMES].values.astype(np.float32)\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"Y_train:\", Y_train.shape)\n",
    "print(\"Core targets:\", CORE_TARGET_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f78620",
   "metadata": {
    "papermill": {
     "duration": 0.004257,
     "end_time": "2026-01-24T07:04:38.003686",
     "exception": false,
     "start_time": "2026-01-24T07:04:37.999429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Train 5 Random Regression Models, 1 Random Forest per target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9f005ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T07:04:38.013313Z",
     "iopub.status.busy": "2026-01-24T07:04:38.013045Z",
     "iopub.status.idle": "2026-01-24T07:05:36.581478Z",
     "shell.execute_reply": "2026-01-24T07:05:36.580690Z"
    },
    "papermill": {
     "duration": 58.57901,
     "end_time": "2026-01-24T07:05:36.586830",
     "exception": false,
     "start_time": "2026-01-24T07:04:38.007820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained RF models (core targets): ['Dry_Total_g', 'GDM_g', 'Dry_Green_g']\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 6 — Training (one RandomForest per CORE target)\n",
    "# ============================================\n",
    "models_rf = {}\n",
    "\n",
    "for i, t in enumerate(CORE_TARGET_NAMES):\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        n_jobs=2,\n",
    "        random_state=42,\n",
    "    )\n",
    "    rf.fit(X_train, Y_train[:, i])\n",
    "    models_rf[t] = rf\n",
    "\n",
    "print(\"Trained RF models (core targets):\", list(models_rf.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5e8c7",
   "metadata": {
    "papermill": {
     "duration": 0.004355,
     "end_time": "2026-01-24T07:05:36.595535",
     "exception": false,
     "start_time": "2026-01-24T07:05:36.591180",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Inference: Extract features once per unique test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78fa4f4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T07:05:36.605756Z",
     "iopub.status.busy": "2026-01-24T07:05:36.605497Z",
     "iopub.status.idle": "2026-01-24T07:05:37.499885Z",
     "shell.execute_reply": "2026-01-24T07:05:37.499330Z"
    },
    "papermill": {
     "duration": 0.901522,
     "end_time": "2026-01-24T07:05:37.501518",
     "exception": false,
     "start_time": "2026-01-24T07:05:36.599996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL7] Using TTA views: 3 (validation/inference-only)\n",
      "[CELL7] Extracting features for TTA view 1/3 ...\n",
      "[CELL4] extract_features: 1 images | batch_size=16 | workers=0\n",
      "[CELL4] Dataset init: 1 images\n",
      "[CELL4] Example path: /kaggle/input/csiro-biomass/test/ID1001187975.jpg\n",
      "[CELL4] total_batches=1\n",
      "[CELL4] First batch: (1, 3, 224, 224)\n",
      "[CELL4] Progress: 1/1\n",
      "[CELL4] features shape: (1, 512)\n",
      "[CELL7] Extracting features for TTA view 2/3 ...\n",
      "[CELL4] extract_features: 1 images | batch_size=16 | workers=0\n",
      "[CELL4] Dataset init: 1 images\n",
      "[CELL4] Example path: /kaggle/input/csiro-biomass/test/ID1001187975.jpg\n",
      "[CELL4] total_batches=1\n",
      "[CELL4] First batch: (1, 3, 224, 224)\n",
      "[CELL4] Progress: 1/1\n",
      "[CELL4] features shape: (1, 512)\n",
      "[CELL7] Extracting features for TTA view 3/3 ...\n",
      "[CELL4] extract_features: 1 images | batch_size=16 | workers=0\n",
      "[CELL4] Dataset init: 1 images\n",
      "[CELL4] Example path: /kaggle/input/csiro-biomass/test/ID1001187975.jpg\n",
      "[CELL4] total_batches=1\n",
      "[CELL4] First batch: (1, 3, 224, 224)\n",
      "[CELL4] Progress: 1/1\n",
      "[CELL4] features shape: (1, 512)\n",
      "[CELL7] Averaging predictions across TTA views for CORE targets ...\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 7 — Inference (extract features once per unique test image) + TTA + 3->5 reconstruction\n",
    "# ============================================\n",
    "needed_cols = {\"sample_id\", \"image_path\", \"target_name\"}\n",
    "if not needed_cols.issubset(test_df.columns):\n",
    "    raise ValueError(f\"test.csv must contain columns {sorted(needed_cols)}\")\n",
    "\n",
    "unique_test_paths = test_df[\"image_path\"].drop_duplicates().tolist()\n",
    "unique_test_abs = [os.path.join(DATA_DIR, p) for p in unique_test_paths]\n",
    "\n",
    "# ----------------------------\n",
    "# TTA (validation/inference-only)\n",
    "# ----------------------------\n",
    "tta_transforms = get_tta_transforms()\n",
    "print(f\"[CELL7] Using TTA views: {len(tta_transforms)} (validation/inference-only)\")\n",
    "\n",
    "# Extract features for each TTA view\n",
    "X_test_views = []\n",
    "for i, tfm in enumerate(tta_transforms):\n",
    "    print(f\"[CELL7] Extracting features for TTA view {i+1}/{len(tta_transforms)} ...\")\n",
    "    X_view = extract_features(unique_test_abs, batch_size=16, transform=tfm)\n",
    "    X_test_views.append(X_view)\n",
    "\n",
    "# ----------------------------\n",
    "# Predict ONLY 3 core targets, then reconstruct full 5\n",
    "# ----------------------------\n",
    "print(\"[CELL7] Averaging predictions across TTA views for CORE targets ...\")\n",
    "pred_core = np.zeros((len(unique_test_paths), len(CORE_TARGET_NAMES)), dtype=np.float32)\n",
    "\n",
    "for j, t in enumerate(CORE_TARGET_NAMES):\n",
    "    view_preds = []\n",
    "    for X_view in X_test_views:\n",
    "        view_preds.append(models_rf[t].predict(X_view))\n",
    "    pred_core[:, j] = np.mean(np.stack(view_preds, axis=0), axis=0).astype(np.float32)\n",
    "\n",
    "# Split core predictions\n",
    "idx_total = CORE_TARGET_NAMES.index(\"Dry_Total_g\")\n",
    "idx_gdm   = CORE_TARGET_NAMES.index(\"GDM_g\")\n",
    "idx_green = CORE_TARGET_NAMES.index(\"Dry_Green_g\")\n",
    "\n",
    "pred_total = np.maximum(0, pred_core[:, idx_total]).astype(np.float32)\n",
    "pred_gdm   = np.maximum(0, pred_core[:, idx_gdm]).astype(np.float32)\n",
    "pred_green = np.maximum(0, pred_core[:, idx_green]).astype(np.float32)\n",
    "\n",
    "# Reconstruct remaining 2 targets (LB-0-57 logic)\n",
    "pred_clover = np.maximum(0, pred_gdm - pred_green).astype(np.float32)  # Clover = max(0, GDM - Green)\n",
    "pred_dead   = np.maximum(0, pred_total - pred_gdm).astype(np.float32)  # Dead   = max(0, Total - GDM)\n",
    "\n",
    "# Build wide 5-target predictions (submission-ready)\n",
    "pred_wide = pd.DataFrame({\n",
    "    \"image_path\": unique_test_paths,\n",
    "    \"Dry_Green_g\": pred_green,\n",
    "    \"Dry_Dead_g\": pred_dead,\n",
    "    \"Dry_Clover_g\": pred_clover,\n",
    "    \"GDM_g\": pred_gdm,\n",
    "    \"Dry_Total_g\": pred_total,\n",
    "})\n",
    "\n",
    "# Ensure expected columns exist and order is consistent\n",
    "pred_wide = pred_wide[[\"image_path\"] + SUBMISSION_TARGET_NAMES]\n",
    "\n",
    "pred_long = pred_wide.melt(\n",
    "    id_vars=\"image_path\",\n",
    "    value_vars=SUBMISSION_TARGET_NAMES,\n",
    "    var_name=\"target_name\",\n",
    "    value_name=\"target\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93447162",
   "metadata": {
    "papermill": {
     "duration": 0.004446,
     "end_time": "2026-01-24T07:05:37.510656",
     "exception": false,
     "start_time": "2026-01-24T07:05:37.506210",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bde66129",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T07:05:37.521120Z",
     "iopub.status.busy": "2026-01-24T07:05:37.520533Z",
     "iopub.status.idle": "2026-01-24T07:05:37.540804Z",
     "shell.execute_reply": "2026-01-24T07:05:37.540013Z"
    },
    "papermill": {
     "duration": 0.02699,
     "end_time": "2026-01-24T07:05:37.542231",
     "exception": false,
     "start_time": "2026-01-24T07:05:37.515241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Preparing submission directly from predictions...\n",
      "Step 2: Constructing sample_id...\n",
      "✅ Submission saved to: /kaggle/working/submission.csv\n",
      "   Shape: (5, 2)\n",
      "   Unique Targets: ['Dry_Green_g' 'Dry_Dead_g' 'Dry_Clover_g' 'GDM_g' 'Dry_Total_g']\n",
      "   Head:\n",
      "                    sample_id     target\n",
      "2  ID1001187975__Dry_Clover_g   0.000000\n",
      "1    ID1001187975__Dry_Dead_g  19.859241\n",
      "0   ID1001187975__Dry_Green_g  21.688047\n",
      "4   ID1001187975__Dry_Total_g  39.719997\n",
      "3         ID1001187975__GDM_g  19.860756\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 8 — SUBMISSION\n",
    "# ============================================\n",
    "pred_long.head()\n",
    "\n",
    "SUBMISSION_PATH = \"/kaggle/working/submission.csv\"\n",
    "\n",
    "def generate_submission_direct(pred_long: pd.DataFrame, output_path: str = SUBMISSION_PATH):\n",
    "    print(\"Step 1: Preparing submission directly from predictions...\")\n",
    "\n",
    "    # We work on a copy to avoid SettingWithCopy warnings\n",
    "    sub = pred_long.copy()\n",
    "\n",
    "    # --- A. CLEAN THE ID ---\n",
    "    # In Cell 3 (Hijack), we set image_path = \"test/image_01.jpg\"\n",
    "    # We need just \"image_01\" for the sample_id.\n",
    "\n",
    "    # 1. Strip directory prefix (get \"image_01.jpg\")\n",
    "    sub['filename'] = sub['image_path'].apply(lambda x: os.path.basename(str(x)))\n",
    "\n",
    "    # 2. Strip extension (get \"image_01\")\n",
    "    sub['image_id'] = sub['filename'].apply(lambda x: os.path.splitext(x)[0])\n",
    "\n",
    "    # --- B. CONSTRUCT SAMPLE_ID ---\n",
    "    # Logic: {ImageID}__{TargetName}\n",
    "    print(\"Step 2: Constructing sample_id...\")\n",
    "    sub['sample_id'] = sub['image_id'] + \"__\" + sub['target_name']\n",
    "\n",
    "    # --- C. CLIP NEGATIVES ---\n",
    "    num_negatives = (sub[\"target\"] < 0).sum()\n",
    "    if num_negatives > 0:\n",
    "        print(f\"⚠️ WARNING: Found {num_negatives} negative predictions. Clipping to 0.\")\n",
    "        sub[\"target\"] = sub[\"target\"].clip(lower=0)\n",
    "\n",
    "    # --- D. FINALIZE ---\n",
    "    final_output = sub[['sample_id', 'target']].copy()\n",
    "\n",
    "    # Sort to be tidy (helps with debugging)\n",
    "    final_output = final_output.sort_values('sample_id')\n",
    "\n",
    "    # Safety Check: Do we have NaNs?\n",
    "    if final_output['target'].isna().any():\n",
    "        print(\"❌ CRITICAL: NaNs found in target. Filling with 0.\")\n",
    "        final_output['target'] = final_output['target'].fillna(0)\n",
    "\n",
    "    # --- E. SAVE ---\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    final_output.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"✅ Submission saved to: {output_path}\")\n",
    "    print(f\"   Shape: {final_output.shape}\")\n",
    "    print(f\"   Unique Targets: {sub['target_name'].unique()}\")\n",
    "    print(\"   Head:\")\n",
    "    print(final_output.head())\n",
    "\n",
    "    return final_output\n",
    "\n",
    "# --- EXECUTE ---\n",
    "try:\n",
    "    submission = generate_submission_direct(pred_long, SUBMISSION_PATH)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error generating submission: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072ecdbb",
   "metadata": {
    "papermill": {
     "duration": 0.004418,
     "end_time": "2026-01-24T07:05:37.551069",
     "exception": false,
     "start_time": "2026-01-24T07:05:37.546651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. SANITY CHECKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f19456f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T07:05:37.562295Z",
     "iopub.status.busy": "2026-01-24T07:05:37.562072Z",
     "iopub.status.idle": "2026-01-24T07:05:38.043692Z",
     "shell.execute_reply": "2026-01-24T07:05:38.042953Z"
    },
    "papermill": {
     "duration": 0.489811,
     "end_time": "2026-01-24T07:05:38.045339",
     "exception": false,
     "start_time": "2026-01-24T07:05:37.555528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "SANITY CHECKS (End-to-End: data -> transforms -> extractor forward -> 3-target -> reconstruct -> 5-target)\n",
      "====================\n",
      "[CELL4] Dataset init: 4 images\n",
      "[CELL4] Example path: /kaggle/input/csiro-biomass/train/ID1011485656.jpg\n",
      "[Sanity] image batch tensor shape: (4, 3, 224, 224)\n",
      "[Sanity] image batch tensor dtype : torch.float32\n",
      "[Sanity] y_true_5 shape: (4, 5)\n",
      "[Sanity] y_true_5 dtype: float32\n",
      "[Sanity] y_pred_3 shape: (4, 3)\n",
      "[Sanity] y_pred_3 dtype: float32\n",
      "[Sanity] y_pred_5 shape: (4, 5)\n",
      "[Sanity] y_pred_5 dtype: float32\n",
      "\n",
      "✅ Sanity checks passed (no exceptions).\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Sanity checks\n",
    "# ============================================\n",
    "print(\"\\n====================\")\n",
    "print(\"SANITY CHECKS (End-to-End: data -> transforms -> extractor forward -> 3-target -> reconstruct -> 5-target)\")\n",
    "print(\"====================\")\n",
    "\n",
    "try:\n",
    "    # Choose a small batch from TRAIN (y_true available)\n",
    "    if len(train_wide) == 0:\n",
    "        raise ValueError(\"train_wide is empty; cannot run sanity checks with y_true.\")\n",
    "\n",
    "    B = min(4, len(train_wide))\n",
    "    sample_paths = train_wide[\"abs_path\"].iloc[:B].tolist()\n",
    "\n",
    "    # Build image batch via the same transform pipeline (resize-only)\n",
    "    ds_sc = ImagePathDataset(sample_paths, transform=IMG_TFM)\n",
    "    dl_sc = DataLoader(\n",
    "        ds_sc,\n",
    "        batch_size=B,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=(DEVICE == \"cuda\"),\n",
    "    )\n",
    "\n",
    "    x = next(iter(dl_sc))  # [B, 3, IMG_SIZE, IMG_SIZE]\n",
    "    print(\"[Sanity] image batch tensor shape:\", tuple(x.shape))\n",
    "    print(\"[Sanity] image batch tensor dtype :\", x.dtype)\n",
    "\n",
    "    # y_true (5 targets) from train_wide if available\n",
    "    y_true_5 = train_wide[SUBMISSION_TARGET_NAMES].iloc[:B].values.astype(np.float32)\n",
    "    print(\"[Sanity] y_true_5 shape:\", y_true_5.shape)\n",
    "    print(\"[Sanity] y_true_5 dtype:\", y_true_5.dtype)\n",
    "\n",
    "    # Model forward (EXTRACTOR) -> features\n",
    "    with torch.no_grad():\n",
    "        x_dev = x.to(DEVICE, non_blocking=True)\n",
    "        feats = EXTRACTOR(x_dev).view(x_dev.size(0), -1)  # [B, 512]\n",
    "    feats_np = feats.cpu().numpy().astype(np.float32)\n",
    "\n",
    "    # 3-target output from RFs\n",
    "    y_pred_3 = np.zeros((B, len(CORE_TARGET_NAMES)), dtype=np.float32)\n",
    "    for j, t in enumerate(CORE_TARGET_NAMES):\n",
    "        y_pred_3[:, j] = models_rf[t].predict(feats_np).astype(np.float32)\n",
    "\n",
    "    print(\"[Sanity] y_pred_3 shape:\", y_pred_3.shape)\n",
    "    print(\"[Sanity] y_pred_3 dtype:\", y_pred_3.dtype)\n",
    "\n",
    "    # Reconstruct to 5 targets (LB-0-57 logic)\n",
    "    pred_total = np.maximum(0, y_pred_3[:, CORE_TARGET_NAMES.index(\"Dry_Total_g\")]).astype(np.float32)\n",
    "    pred_gdm   = np.maximum(0, y_pred_3[:, CORE_TARGET_NAMES.index(\"GDM_g\")]).astype(np.float32)\n",
    "    pred_green = np.maximum(0, y_pred_3[:, CORE_TARGET_NAMES.index(\"Dry_Green_g\")]).astype(np.float32)\n",
    "\n",
    "    pred_clover = np.maximum(0, pred_gdm - pred_green).astype(np.float32)\n",
    "    pred_dead   = np.maximum(0, pred_total - pred_gdm).astype(np.float32)\n",
    "\n",
    "    y_pred_5 = np.stack([pred_green, pred_dead, pred_clover, pred_gdm, pred_total], axis=1).astype(np.float32)\n",
    "\n",
    "    print(\"[Sanity] y_pred_5 shape:\", y_pred_5.shape)\n",
    "    print(\"[Sanity] y_pred_5 dtype:\", y_pred_5.dtype)\n",
    "\n",
    "    # Confirm fixed tensor shape requirement\n",
    "    assert tuple(x.shape[1:]) == (3, IMG_SIZE, IMG_SIZE), \"Unexpected image tensor shape after transforms.\"\n",
    "\n",
    "    print(\"\\n✅ Sanity checks passed (no exceptions).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\n❌ Sanity checks failed with exception:\")\n",
    "    print(type(e).__name__, str(e))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e281bbec",
   "metadata": {
    "papermill": {
     "duration": 0.004739,
     "end_time": "2026-01-24T07:05:38.055165",
     "exception": false,
     "start_time": "2026-01-24T07:05:38.050426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaebf68",
   "metadata": {
    "papermill": {
     "duration": 0.004551,
     "end_time": "2026-01-24T07:05:38.064350",
     "exception": false,
     "start_time": "2026-01-24T07:05:38.059799",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Change log\n",
    "\n",
    "## Kaggle version name (suggested)\n",
    "- **kaggle_trang_attempt_current_v3_letterbox_core3_reconstruct5_tta**\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of changes\n",
    "\n",
    "### A) Letterboxing added (SquarePad + Resize)\n",
    "**Modified cells**\n",
    "- **Cell 2 — Letterboxing**\n",
    "  - Added optimized `SquarePad(fill=0, padding_mode=\"constant\")` for symmetric pad-to-square.\n",
    "  - Added `build_letterbox_resize_transform(IMG_SIZE, mean, std, fill)` to centralize the preprocessing pipeline.\n",
    "\n",
    "- **Cell 3 — ResNet18 feature extractor transform**\n",
    "  - Updated `IMG_TFM` to:\n",
    "    - `SquarePad()` → `Resize((IMG_SIZE, IMG_SIZE))` → `ToTensor()` → `Normalize(mean,std)`\n",
    "  - No other changes to model/weights loading.\n",
    "\n",
    "- **Cell 4 — Duplicate ResNet18 setup**\n",
    "  - Mirrored the same `IMG_TFM` update to keep behavior consistent across both duplicated setup cells.\n",
    "\n",
    "- **Cell 5 — TTA transforms (validation/inference-only)**\n",
    "  - Updated TTA pipelines to use the same letterbox+resize base transform (via the builder), with deterministic flips layered on top:\n",
    "    - original\n",
    "    - horizontal flip\n",
    "    - vertical flip\n",
    "\n",
    "**Net effect**\n",
    "- All image preprocessing that feeds the feature extractor now preserves aspect ratio via padding-to-square before resizing.\n",
    "- No padding-based resizing is used outside this explicit `SquarePad` step (i.e., letterboxing behavior is centralized and consistent).\n",
    "\n",
    "---\n",
    "\n",
    "### B) Core Strategy retained (3-target training → 5-target reconstruction)\n",
    "**Already implemented and unchanged by this update**\n",
    "- Train/predict only **3 targets**:\n",
    "  - `Dry_Total_g`, `GDM_g`, `Dry_Green_g`\n",
    "- Reconstruct the remaining **2 targets** during validation/inference:\n",
    "  - `Dry_Clover_g = max(0, GDM_g - Dry_Green_g)`\n",
    "  - `Dry_Dead_g   = max(0, Dry_Total_g - GDM_g)`\n",
    "- Submission still outputs **all 5 targets** in the required naming/order.\n",
    "\n",
    "---\n",
    "\n",
    "## Assumptions\n",
    "- `IMG_SIZE` is the single canonical image size used everywhere.\n",
    "- Letterboxing uses constant black padding (`fill=0`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8055ceb2",
   "metadata": {
    "papermill": {
     "duration": 0.004607,
     "end_time": "2026-01-24T07:05:38.073437",
     "exception": false,
     "start_time": "2026-01-24T07:05:38.068830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d2f44b",
   "metadata": {
    "papermill": {
     "duration": 0.00443,
     "end_time": "2026-01-24T07:05:38.082444",
     "exception": false,
     "start_time": "2026-01-24T07:05:38.078014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10. EXPORT AUGMENTED IMAGES SO THAT CHU NHAT CAN FIND BEST PARAMETERS FOR RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34513faa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T07:05:38.093285Z",
     "iopub.status.busy": "2026-01-24T07:05:38.093035Z",
     "iopub.status.idle": "2026-01-24T07:05:38.097505Z",
     "shell.execute_reply": "2026-01-24T07:05:38.096942Z"
    },
    "papermill": {
     "duration": 0.011803,
     "end_time": "2026-01-24T07:05:38.098798",
     "exception": false,
     "start_time": "2026-01-24T07:05:38.086995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ============================================\n",
    "# #  Export augmented TRAIN images (original / hflip / vflip) WITHOUT resizing\n",
    "# # + Print sizes for a sample (original vs hflip vs vflip)\n",
    "# # Writes to: /kaggle/working/augmented_train_images/{original,hflip,vflip}/\n",
    "# # ============================================\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "# from PIL import Image\n",
    "# from torchvision import transforms\n",
    "\n",
    "# # ---- Config ----\n",
    "# OUTPUT_DIR = Path(\"/kaggle/working/augmented_train_images\")  # ✅ matches zipping cell\n",
    "# MAX_IMAGES = None          # set to an int (e.g., 500) to limit exports; None = export all\n",
    "# PRINT_FIRST_N = 10         # how many images to print size diagnostics for\n",
    "\n",
    "# # ---- Force training image source ----\n",
    "# if \"train_wide\" in globals() and \"abs_path\" in train_wide.columns and len(train_wide) > 0:\n",
    "#     export_abs_paths = train_wide[\"abs_path\"].tolist()\n",
    "#     export_rel_paths = train_wide[\"image_path\"].tolist()\n",
    "#     print(f\"[Export] Using TRAIN images: {len(export_abs_paths)}\")\n",
    "# else:\n",
    "#     raise RuntimeError(\n",
    "#         \"train_wide['abs_path'] not found. Run the cell that builds train_wide and its abs_path first.\"\n",
    "#     )\n",
    "\n",
    "# # Apply MAX_IMAGES cap (optional)\n",
    "# if isinstance(MAX_IMAGES, int) and MAX_IMAGES > 0:\n",
    "#     export_abs_paths = export_abs_paths[:MAX_IMAGES]\n",
    "#     export_rel_paths = export_rel_paths[:MAX_IMAGES]\n",
    "#     print(f\"[Export] Capped to MAX_IMAGES={MAX_IMAGES}\")\n",
    "\n",
    "# # ---- Save-friendly augmentation pipelines (PIL outputs; NO Normalize/ToTensor; NO padding; NO resizing) ----\n",
    "# # Goal: keep image sizes exactly as original.\n",
    "# save_views = {\n",
    "#     \"original\": transforms.Compose([]),  # identity\n",
    "#     \"hflip\": transforms.Compose([transforms.RandomHorizontalFlip(p=1.0)]),\n",
    "#     \"vflip\": transforms.Compose([transforms.RandomVerticalFlip(p=1.0)]),\n",
    "# }\n",
    "\n",
    "# # ---- Create output folders ----\n",
    "# OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# for view_name in save_views.keys():\n",
    "#     (OUTPUT_DIR / view_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# print(f\"[Export] Writing augmented TRAIN images (no resize) to: {OUTPUT_DIR}\")\n",
    "\n",
    "# # ---- Export loop + size diagnostics ----\n",
    "# n_ok, n_fail = 0, 0\n",
    "# printed = 0\n",
    "\n",
    "# for i, (abs_p, rel_p) in enumerate(zip(export_abs_paths, export_rel_paths), start=1):\n",
    "#     try:\n",
    "#         img = Image.open(abs_p).convert(\"RGB\")\n",
    "#         orig_size = img.size  # (W, H)\n",
    "\n",
    "#         base_name = Path(rel_p).name\n",
    "#         stem = Path(base_name).stem\n",
    "#         ext = Path(base_name).suffix.lower()\n",
    "#         if ext not in {\".jpg\", \".jpeg\", \".png\"}:\n",
    "#             ext = \".jpg\"\n",
    "\n",
    "#         sizes = {\"original\": None, \"hflip\": None, \"vflip\": None}\n",
    "\n",
    "#         for view_name, tfm in save_views.items():\n",
    "#             out_img = tfm(img)\n",
    "#             sizes[view_name] = out_img.size  # (W, H)\n",
    "#             out_path = OUTPUT_DIR / view_name / f\"{stem}__{view_name}{ext}\"\n",
    "\n",
    "#             # Save with appropriate settings (avoid passing invalid args for PNG)\n",
    "#             if ext in {\".jpg\", \".jpeg\"}:\n",
    "#                 out_img.save(out_path, quality=95)\n",
    "#             else:\n",
    "#                 out_img.save(out_path)\n",
    "\n",
    "#         # Print size diagnostics for first PRINT_FIRST_N images\n",
    "#         if printed < PRINT_FIRST_N:\n",
    "#             print(f\"\\n[Size] {i}: {base_name}\")\n",
    "#             print(f\"  - original: {orig_size}\")\n",
    "#             print(f\"  - hflip   : {sizes['hflip']}\")\n",
    "#             print(f\"  - vflip   : {sizes['vflip']}\")\n",
    "#             printed += 1\n",
    "\n",
    "#         # Hard check: sizes must match\n",
    "#         if not (orig_size == sizes[\"hflip\"] == sizes[\"vflip\"]):\n",
    "#             print(f\"[WARN] Size mismatch for {base_name}: orig={orig_size}, hflip={sizes['hflip']}, vflip={sizes['vflip']}\")\n",
    "\n",
    "#         n_ok += 1\n",
    "#         if i == 1 or i % 200 == 0 or i == len(export_abs_paths):\n",
    "#             print(f\"[Export] {i}/{len(export_abs_paths)} done\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         n_fail += 1\n",
    "#         if n_fail <= 10:\n",
    "#             print(f\"[Export][WARN] Failed on: {abs_p} | {type(e).__name__}: {e}\")\n",
    "\n",
    "# print(f\"\\n[Export] Completed. Success={n_ok}, Failed={n_fail}\")\n",
    "# print(f\"[Export] Output folder: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf03617",
   "metadata": {
    "papermill": {
     "duration": 0.004548,
     "end_time": "2026-01-24T07:05:38.107969",
     "exception": false,
     "start_time": "2026-01-24T07:05:38.103421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 11 DOWNLOAD OUTPUT AUGMENTED IMAGES BY USING SHUTIL LIBRARY IN PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f41e2789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T07:05:38.118580Z",
     "iopub.status.busy": "2026-01-24T07:05:38.118034Z",
     "iopub.status.idle": "2026-01-24T07:05:38.121258Z",
     "shell.execute_reply": "2026-01-24T07:05:38.120724Z"
    },
    "papermill": {
     "duration": 0.009984,
     "end_time": "2026-01-24T07:05:38.122505",
     "exception": false,
     "start_time": "2026-01-24T07:05:38.112521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ============================================\n",
    "# # Zip the exported folder from CODE CELL 1\n",
    "# # (Smooth transition: uses the SAME directory path)\n",
    "# # ============================================\n",
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Define the directory you want to zip (the augmented TRAIN images folder created above)\n",
    "# directory_to_zip = str(OUTPUT_DIR)  # ✅ directly reuses OUTPUT_DIR from Cell 1\n",
    "\n",
    "# # Define the output filename (without extension)\n",
    "# output_filename = 'augmented_train_images_archive'\n",
    "\n",
    "# # Create the zip archive\n",
    "# shutil.make_archive(output_filename, 'zip', directory_to_zip)\n",
    "\n",
    "# print(f\"Archived {directory_to_zip} to {output_filename}.zip\")\n",
    "# print(\"✅ Zip saved at:\", os.path.join(\"/kaggle/working\", f\"{output_filename}.zip\"))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "datasetId": 2847,
     "sourceId": 4958,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 115.288919,
   "end_time": "2026-01-24T07:05:41.189369",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-24T07:03:45.900450",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
