{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43b3a330",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-06T19:05:41.387107Z",
     "iopub.status.busy": "2026-01-06T19:05:41.386657Z",
     "iopub.status.idle": "2026-01-06T19:05:41.394134Z",
     "shell.execute_reply": "2026-01-06T19:05:41.392792Z"
    },
    "papermill": {
     "duration": 0.01682,
     "end_time": "2026-01-06T19:05:41.396334",
     "exception": false,
     "start_time": "2026-01-06T19:05:41.379514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2bb4dbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T19:05:41.406260Z",
     "iopub.status.busy": "2026-01-06T19:05:41.405954Z",
     "iopub.status.idle": "2026-01-06T19:05:41.417606Z",
     "shell.execute_reply": "2026-01-06T19:05:41.416440Z"
    },
    "papermill": {
     "duration": 0.019628,
     "end_time": "2026-01-06T19:05:41.419828",
     "exception": false,
     "start_time": "2026-01-06T19:05:41.400200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nKaggle to GitHub Sync Configuration\\nRepository: CODES-ON-KAGGLE\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "OLD CODE: VER 2 FROM GEMINI\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Kaggle to GitHub Sync Configuration\n",
    "Repository: CODES-ON-KAGGLE\n",
    "\"\"\"\n",
    "# from pathlib import Path\n",
    "# import os\n",
    "\n",
    "# # --- KAGGLE CONFIGURATION ---\n",
    "\n",
    "# # 1. Define the Base Directory using pathlib\n",
    "# # Kaggle inputs are read-only and located in /kaggle/input\n",
    "# BASE_DIR = Path('/kaggle/input') \n",
    "\n",
    "# # 2. Define the Dataset Name \n",
    "# # Change 'project-name' to the folder name found in /kaggle/input\n",
    "# # NOTE: 'project-name' can be found by looking at the URL when accessing Kaggle Competitions\n",
    "# DATASET_NAME = 'csiro-biomass'\n",
    "# DATASET_PATH = BASE_DIR / DATASET_NAME\n",
    "\n",
    "# # 3. Define Paths to CSVs and Image Directories\n",
    "# # We use the / operator for clean, cross-platform path joining\n",
    "# TRAIN_CSV_PATH = DATASET_PATH / 'train.csv'\n",
    "# TEST_CSV_PATH = DATASET_PATH / 'test.csv'\n",
    "\n",
    "# TRAIN_IMAGES_ROOT = DATASET_PATH / 'train'\n",
    "# TEST_IMAGE_ROOT = DATASET_PATH / 'test'\n",
    "\n",
    "# # 4. Verification Logic\n",
    "# def verify_paths():\n",
    "#     paths_to_check = {\n",
    "#         \"Training CSV\": TRAIN_CSV_PATH,\n",
    "#         \"Testing CSV\": TEST_CSV_PATH,\n",
    "#         \"Train Images Root\": TRAIN_IMAGES_ROOT,\n",
    "#         \"Test Image Root\"  : TEST_IMAGE_ROOT\n",
    "#     }\n",
    "    \n",
    "#     print(f\"--- Path Verification for {DATASET_NAME} ---\")\n",
    "#     for name, path in paths_to_check.items():\n",
    "#         if path.exists():\n",
    "#             print(f\"✅ {name} found at: {path}\")\n",
    "#         else:\n",
    "#             print(f\"❌ {name} NOT found at: {path}\")\n",
    "            \n",
    "#     # If the main folder isn't found, list what is available\n",
    "#     if not DATASET_PATH.exists():\n",
    "#         print(f\"\\nAvailable directories in {BASE_DIR}:\")\n",
    "#         print([p.name for p in BASE_DIR.iterdir() if p.is_dir()])\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     verify_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09c685c",
   "metadata": {
    "papermill": {
     "duration": 0.003526,
     "end_time": "2026-01-06T19:05:41.427027",
     "exception": false,
     "start_time": "2026-01-06T19:05:41.423501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. IMPORT LIBARIES + PATHS + REPRODUCBILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bc62f76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T19:05:41.436119Z",
     "iopub.status.busy": "2026-01-06T19:05:41.435779Z",
     "iopub.status.idle": "2026-01-06T19:06:00.185608Z",
     "shell.execute_reply": "2026-01-06T19:06:00.183955Z"
    },
    "papermill": {
     "duration": 18.757934,
     "end_time": "2026-01-06T19:06:00.188337",
     "exception": false,
     "start_time": "2026-01-06T19:05:41.430403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DATA_DIR: /kaggle/input/csiro-biomass\n",
      "✅ WEIGHTS_DIR: /kaggle/input/pretrained-pytorch-models\n",
      "✅ DEVICE: cpu\n",
      "✅ Train CSV: /kaggle/input/csiro-biomass/train.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 1 — Imports + Paths + Reprocibility\n",
    "# ============================================\n",
    "\"\"\"\n",
    "Kaggle to GitHub Sync Configuration\n",
    "Repository: CODES-ON-KAGGLE\n",
    "\"\"\"\n",
    "# Import\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ✅ Competition directory (as you stated)\n",
    "DATA_DIR = \"/kaggle/input/csiro-biomass\"\n",
    "\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\n",
    "TEST_CSV  = os.path.join(DATA_DIR, \"test.csv\")\n",
    "SAMPLE_SUB_CSV = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "# ✅ Pretrained weights dataset input for model resnet18\n",
    "WEIGHTS_DIR = \"/kaggle/input/pretrained-pytorch-models\"\n",
    "\n",
    "# Inference\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# BATCH_SIZE = 1 # INCREASE BATCH SIZE WHEN INFERENCE\n",
    "# NUM_WORKERS = 1 # 2 IN MY PRIOR DELETED VERSION\n",
    "# N_FOLDS = 5 \n",
    "\n",
    "# Safety checks (fail fast if something isn't mounted)\n",
    "for p in [DATA_DIR, WEIGHTS_DIR, TRAIN_CSV, TEST_CSV, SAMPLE_SUB_CSV]:\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Missing expected path: {p}\")\n",
    "\n",
    "print(\"✅ DATA_DIR:\", DATA_DIR)\n",
    "print(\"✅ WEIGHTS_DIR:\", WEIGHTS_DIR)\n",
    "print(\"✅ DEVICE:\", DEVICE)\n",
    "print(\"✅ Train CSV:\", TRAIN_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238796ba",
   "metadata": {
    "papermill": {
     "duration": 0.003762,
     "end_time": "2026-01-06T19:06:00.195798",
     "exception": false,
     "start_time": "2026-01-06T19:06:00.192036",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. SETUP WEIGHTS FOR THE OFFLINE RESNET 18\n",
    "Since we run code when the Internet is off, we have to manually input a dataset on Kaggle\n",
    "that contains ResNet18 ImageNet weights.\n",
    "The ResNet18 ImageNet weights will be named resnet18-5c106cde.pth or resnet18.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd2df6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T19:06:00.205615Z",
     "iopub.status.busy": "2026-01-06T19:06:00.205077Z",
     "iopub.status.idle": "2026-01-06T19:06:01.122158Z",
     "shell.execute_reply": "2026-01-06T19:06:01.120233Z"
    },
    "papermill": {
     "duration": 0.926026,
     "end_time": "2026-01-06T19:06:01.125461",
     "exception": false,
     "start_time": "2026-01-06T19:06:00.199435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] weights_dir: /kaggle/input/pretrained-pytorch-models\n",
      "[DEBUG] resnet18 candidates found: 1\n",
      "[DEBUG] first few candidates: ['/kaggle/input/pretrained-pytorch-models/resnet18-5c106cde.pth']\n",
      "[DEBUG] RESNET18_WEIGHTS selected: /kaggle/input/pretrained-pytorch-models/resnet18-5c106cde.pth\n",
      "[DEBUG] Feature extractor ready.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 2 — Model / Weights Setup (ResNet18 offline) + Robust Loader + Debug Prints\n",
    "# ============================================\n",
    "def find_resnet18_weights(weights_dir: str) -> str:\n",
    "    print(\"[DEBUG] weights_dir:\", weights_dir)\n",
    "\n",
    "    wd = Path(weights_dir)\n",
    "    if not wd.exists():\n",
    "        raise FileNotFoundError(f\"Missing weights dataset. Expected directory: {weights_dir}\")\n",
    "\n",
    "    candidates = []\n",
    "    for p in wd.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in {\".pth\", \".pt\"} and \"resnet18\" in p.name.lower():\n",
    "            candidates.append(str(p))\n",
    "\n",
    "    print(\"[DEBUG] resnet18 candidates found:\", len(candidates))\n",
    "    if candidates:\n",
    "        print(\"[DEBUG] first few candidates:\", candidates[:5])\n",
    "\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"No resnet18 .pth/.pt file found under {weights_dir}\")\n",
    "\n",
    "    candidates_sorted = sorted(\n",
    "        candidates,\n",
    "        key=lambda x: (0 if re.search(r\"resnet18-.*\\.pth$\", os.path.basename(x).lower()) else 1, len(x))\n",
    "    )\n",
    "    chosen = candidates_sorted[0]\n",
    "    print(\"[DEBUG] RESNET18_WEIGHTS selected:\", chosen)\n",
    "    return chosen\n",
    "\n",
    "RESNET18_WEIGHTS = find_resnet18_weights(WEIGHTS_DIR)\n",
    "\n",
    "def _to_state_dict(obj):\n",
    "    \"\"\"Convert various checkpoint formats into a state_dict dict.\"\"\"\n",
    "    if isinstance(obj, nn.Module):\n",
    "        return obj.state_dict()\n",
    "    if isinstance(obj, dict):\n",
    "        if \"state_dict\" in obj and isinstance(obj[\"state_dict\"], dict):\n",
    "            return obj[\"state_dict\"]\n",
    "        # If it already looks like a state_dict\n",
    "        if all(isinstance(k, str) for k in obj.keys()):\n",
    "            return obj\n",
    "    raise TypeError(f\"Unsupported checkpoint type: {type(obj)}\")\n",
    "\n",
    "def build_resnet18_feature_extractor(weights_path: str, device: str):\n",
    "    model = models.resnet18(weights=None)  # no internet download\n",
    "    raw = torch.load(weights_path, map_location=\"cpu\", weights_only=False)\n",
    "    state = _to_state_dict(raw)\n",
    "\n",
    "    cleaned = {}\n",
    "    for k, v in state.items():\n",
    "        k2 = k\n",
    "        for prefix in (\"module.\", \"model.\"):\n",
    "            if k2.startswith(prefix):\n",
    "                k2 = k2[len(prefix):]\n",
    "        cleaned[k2] = v\n",
    "\n",
    "    missing, unexpected = model.load_state_dict(cleaned, strict=False)\n",
    "    if missing:\n",
    "        print(\"Warning: missing keys when loading weights:\", missing[:5], (\"...\" if len(missing) > 5 else \"\"))\n",
    "    if unexpected:\n",
    "        print(\"Warning: unexpected keys when loading weights:\", unexpected[:5], (\"...\" if len(unexpected) > 5 else \"\"))\n",
    "\n",
    "    extractor = nn.Sequential(*list(model.children())[:-1]).to(device)\n",
    "    extractor.eval()\n",
    "    for p in extractor.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=(0.485, 0.456, 0.406),\n",
    "            std=(0.229, 0.224, 0.225),\n",
    "        ),\n",
    "    ])\n",
    "    return extractor, tfm\n",
    "\n",
    "EXTRACTOR, IMG_TFM = build_resnet18_feature_extractor(RESNET18_WEIGHTS, DEVICE)\n",
    "print(\"[DEBUG] Feature extractor ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc822ef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T19:06:01.138449Z",
     "iopub.status.busy": "2026-01-06T19:06:01.137611Z",
     "iopub.status.idle": "2026-01-06T19:06:01.293230Z",
     "shell.execute_reply": "2026-01-06T19:06:01.292103Z"
    },
    "papermill": {
     "duration": 0.165607,
     "end_time": "2026-01-06T19:06:01.295782",
     "exception": false,
     "start_time": "2026-01-06T19:06:01.130175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df: (1785, 9)\n",
      "test_df : (5, 3)\n",
      "sample_submission: (5, 2)\n",
      "train_wide: (357, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>target_name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>Dry_Clover_g</th>\n",
       "      <th>Dry_Dead_g</th>\n",
       "      <th>Dry_Green_g</th>\n",
       "      <th>Dry_Total_g</th>\n",
       "      <th>GDM_g</th>\n",
       "      <th>abs_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>31.9984</td>\n",
       "      <td>16.2751</td>\n",
       "      <td>48.2735</td>\n",
       "      <td>16.2750</td>\n",
       "      <td>/kaggle/input/csiro-biomass/train/ID1011485656...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train/ID1012260530.jpg</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.6000</td>\n",
       "      <td>7.6000</td>\n",
       "      <td>7.6000</td>\n",
       "      <td>/kaggle/input/csiro-biomass/train/ID1012260530...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/ID1025234388.jpg</td>\n",
       "      <td>6.0500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0500</td>\n",
       "      <td>6.0500</td>\n",
       "      <td>/kaggle/input/csiro-biomass/train/ID1025234388...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train/ID1028611175.jpg</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>30.9703</td>\n",
       "      <td>24.2376</td>\n",
       "      <td>55.2079</td>\n",
       "      <td>24.2376</td>\n",
       "      <td>/kaggle/input/csiro-biomass/train/ID1028611175...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train/ID1035947949.jpg</td>\n",
       "      <td>0.4343</td>\n",
       "      <td>23.2239</td>\n",
       "      <td>10.5261</td>\n",
       "      <td>34.1844</td>\n",
       "      <td>10.9605</td>\n",
       "      <td>/kaggle/input/csiro-biomass/train/ID1035947949...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "target_name              image_path  Dry_Clover_g  Dry_Dead_g  Dry_Green_g  \\\n",
       "0            train/ID1011485656.jpg        0.0000     31.9984      16.2751   \n",
       "1            train/ID1012260530.jpg        0.0000      0.0000       7.6000   \n",
       "2            train/ID1025234388.jpg        6.0500      0.0000       0.0000   \n",
       "3            train/ID1028611175.jpg        0.0000     30.9703      24.2376   \n",
       "4            train/ID1035947949.jpg        0.4343     23.2239      10.5261   \n",
       "\n",
       "target_name  Dry_Total_g    GDM_g  \\\n",
       "0                48.2735  16.2750   \n",
       "1                 7.6000   7.6000   \n",
       "2                 6.0500   6.0500   \n",
       "3                55.2079  24.2376   \n",
       "4                34.1844  10.9605   \n",
       "\n",
       "target_name                                           abs_path  \n",
       "0            /kaggle/input/csiro-biomass/train/ID1011485656...  \n",
       "1            /kaggle/input/csiro-biomass/train/ID1012260530...  \n",
       "2            /kaggle/input/csiro-biomass/train/ID1025234388...  \n",
       "3            /kaggle/input/csiro-biomass/train/ID1028611175...  \n",
       "4            /kaggle/input/csiro-biomass/train/ID1035947949...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 3 — Data Loading (train/test/sample_submission)\n",
    "# ============================================\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df  = pd.read_csv(TEST_CSV)\n",
    "sample_sub = pd.read_csv(SAMPLE_SUB_CSV)\n",
    "\n",
    "print(\"train_df:\", train_df.shape)\n",
    "print(\"test_df :\", test_df.shape)\n",
    "print(\"sample_submission:\", sample_sub.shape)\n",
    "\n",
    "TARGET_NAMES = [\"Dry_Clover_g\", \"Dry_Dead_g\", \"Dry_Green_g\", \"Dry_Total_g\", \"GDM_g\"]\n",
    "\n",
    "def make_train_wide(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      image_path + TARGET_NAMES columns\n",
    "    Supports:\n",
    "      - modern long format: image_path, target_name, target\n",
    "      - fallback: image_path, target (assumes 5-row blocks per image)\n",
    "    \"\"\"\n",
    "    if {\"image_path\", \"target_name\", \"target\"}.issubset(df.columns):\n",
    "        wide = (\n",
    "            df.pivot_table(index=\"image_path\", columns=\"target_name\", values=\"target\", aggfunc=\"first\")\n",
    "              .reset_index()\n",
    "        )\n",
    "        for t in TARGET_NAMES:\n",
    "            if t not in wide.columns:\n",
    "                wide[t] = np.nan\n",
    "        return wide[[\"image_path\"] + TARGET_NAMES]\n",
    "\n",
    "    if {\"image_path\", \"target\"}.issubset(df.columns):\n",
    "        paths = df[\"image_path\"].values\n",
    "        y = df[\"target\"].values\n",
    "        if len(df) % 5 != 0:\n",
    "            raise ValueError(\"Fallback parsing expected train rows multiple of 5.\")\n",
    "        unique_paths = paths[0::5]\n",
    "        wide = pd.DataFrame({\"image_path\": unique_paths})\n",
    "        for i, t in enumerate(TARGET_NAMES):\n",
    "            wide[t] = y[i::5]\n",
    "        return wide\n",
    "\n",
    "    raise ValueError(\"train.csv must have either (image_path,target_name,target) or (image_path,target).\")\n",
    "\n",
    "train_wide = make_train_wide(train_df)\n",
    "train_wide[\"abs_path\"] = train_wide[\"image_path\"].apply(lambda p: os.path.join(DATA_DIR, p))\n",
    "train_wide = train_wide.dropna(subset=TARGET_NAMES).reset_index(drop=True)\n",
    "\n",
    "print(\"train_wide:\", train_wide.shape)\n",
    "train_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ee6d3a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T19:06:01.309958Z",
     "iopub.status.busy": "2026-01-06T19:06:01.308649Z",
     "iopub.status.idle": "2026-01-06T19:06:01.318377Z",
     "shell.execute_reply": "2026-01-06T19:06:01.316677Z"
    },
    "papermill": {
     "duration": 0.020038,
     "end_time": "2026-01-06T19:06:01.320567",
     "exception": false,
     "start_time": "2026-01-06T19:06:01.300529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] train_df columns: ['sample_id', 'image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm', 'target_name', 'target']\n",
      "[DEBUG] test_df columns : ['sample_id', 'image_path', 'target_name']\n",
      "[DEBUG] train_wide rows : 357\n"
     ]
    }
   ],
   "source": [
    "# Fail-fast checks (put after train_wide is built)\n",
    "print(\"[DEBUG] train_df columns:\", list(train_df.columns))\n",
    "print(\"[DEBUG] test_df columns :\", list(test_df.columns))\n",
    "print(\"[DEBUG] train_wide rows :\", len(train_wide))\n",
    "\n",
    "if len(train_wide) == 0:\n",
    "    raise ValueError(\"train_wide is empty. train.csv parsing likely mismatched the actual format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6205e24",
   "metadata": {
    "papermill": {
     "duration": 0.004469,
     "end_time": "2026-01-06T19:06:01.329770",
     "exception": false,
     "start_time": "2026-01-06T19:06:01.325301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. SET UP A CLASS OF RESNET18 FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b8af775",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T19:06:01.343965Z",
     "iopub.status.busy": "2026-01-06T19:06:01.343181Z",
     "iopub.status.idle": "2026-01-06T19:06:01.356285Z",
     "shell.execute_reply": "2026-01-06T19:06:01.355184Z"
    },
    "papermill": {
     "duration": 0.023681,
     "end_time": "2026-01-06T19:06:01.358338",
     "exception": false,
     "start_time": "2026-01-06T19:06:01.334657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 4 — Feature Extraction Utilities (stable for Kaggle submission)\n",
    "# ============================================\n",
    "BATCH_SIZE = 2  # keep your setting\n",
    "\n",
    "NUM_WORKERS = 0  # ✅ stable setting for Kaggle submission runs\n",
    "\n",
    "class ImagePathDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = list(image_paths)\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f\"[CELL4] Dataset init: {len(self.image_paths)} images\")\n",
    "        if self.image_paths:\n",
    "            print(\"[CELL4] Example path:\", self.image_paths[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.image_paths[idx]\n",
    "        try:\n",
    "            img = Image.open(p).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            if idx < 3:\n",
    "                print(f\"[CELL4][WARN] Open failed idx={idx}: {p} | {type(e).__name__}\")\n",
    "            img = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(image_paths, batch_size=BATCH_SIZE):\n",
    "    print(f\"[CELL4] extract_features: {len(image_paths)} images | batch_size={batch_size} | workers={NUM_WORKERS}\")\n",
    "\n",
    "    ds = ImagePathDataset(image_paths, transform=IMG_TFM)\n",
    "    loader = DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,           # ✅ stable\n",
    "        pin_memory=(DEVICE == \"cuda\"),\n",
    "    )\n",
    "\n",
    "    feats = []\n",
    "    total_batches = len(loader)\n",
    "    print(f\"[CELL4] total_batches={total_batches}\")\n",
    "\n",
    "    for b_idx, batch in enumerate(loader):\n",
    "        if b_idx == 0:\n",
    "            print(\"[CELL4] First batch:\", tuple(batch.shape))\n",
    "\n",
    "        batch = batch.to(DEVICE, non_blocking=True)\n",
    "        out = EXTRACTOR(batch)              # [B, 512, 1, 1]\n",
    "        out = out.view(out.size(0), -1)     # [B, 512]\n",
    "        feats.append(out.cpu().numpy())\n",
    "\n",
    "        if (b_idx + 1) == 1 or (b_idx + 1) == total_batches or (b_idx + 1) % max(1, total_batches // 5) == 0:\n",
    "            print(f\"[CELL4] Progress: {b_idx+1}/{total_batches}\")\n",
    "\n",
    "    feats_np = np.vstack(feats) if feats else np.empty((0, 512), dtype=np.float32)\n",
    "    print(\"[CELL4] features shape:\", feats_np.shape)\n",
    "    return feats_np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e996876",
   "metadata": {
    "papermill": {
     "duration": 0.004036,
     "end_time": "2026-01-06T19:06:01.366749",
     "exception": false,
     "start_time": "2026-01-06T19:06:01.362713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Extract Train Images Features + Make sure the function in Cell 4 runs correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe45a185",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T19:06:01.376553Z",
     "iopub.status.busy": "2026-01-06T19:06:01.376189Z",
     "iopub.status.idle": "2026-01-06T19:06:48.047060Z",
     "shell.execute_reply": "2026-01-06T19:06:48.045066Z"
    },
    "papermill": {
     "duration": 46.679,
     "end_time": "2026-01-06T19:06:48.049662",
     "exception": false,
     "start_time": "2026-01-06T19:06:01.370662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL4] extract_features: 357 images | batch_size=64 | workers=0\n",
      "[CELL4] Dataset init: 357 images\n",
      "[CELL4] Example path: /kaggle/input/csiro-biomass/train/ID1011485656.jpg\n",
      "[CELL4] total_batches=6\n",
      "[CELL4] First batch: (64, 3, 224, 224)\n",
      "[CELL4] Progress: 1/6\n",
      "[CELL4] Progress: 2/6\n",
      "[CELL4] Progress: 3/6\n",
      "[CELL4] Progress: 4/6\n",
      "[CELL4] Progress: 5/6\n",
      "[CELL4] Progress: 6/6\n",
      "[CELL4] features shape: (357, 512)\n",
      "X_train: (357, 512)\n",
      "Y_train: (357, 5)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 5 — Extract Train Features\n",
    "# ============================================\n",
    "X_train = extract_features(train_wide[\"abs_path\"].tolist(), batch_size=64)\n",
    "Y_train = train_wide[TARGET_NAMES].values.astype(np.float32)\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"Y_train:\", Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc4f93f",
   "metadata": {
    "papermill": {
     "duration": 0.004674,
     "end_time": "2026-01-06T19:06:48.058987",
     "exception": false,
     "start_time": "2026-01-06T19:06:48.054313",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Train 5 Random Regression Models, 1 Random Forest per target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e981431",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T19:06:48.070160Z",
     "iopub.status.busy": "2026-01-06T19:06:48.069775Z",
     "iopub.status.idle": "2026-01-06T19:08:17.011153Z",
     "shell.execute_reply": "2026-01-06T19:08:17.009850Z"
    },
    "papermill": {
     "duration": 88.953528,
     "end_time": "2026-01-06T19:08:17.017080",
     "exception": false,
     "start_time": "2026-01-06T19:06:48.063552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained RF models: ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 6 — Training (one RandomForest per target)\n",
    "# ============================================\n",
    "models_rf = {}\n",
    "\n",
    "for i, t in enumerate(TARGET_NAMES):\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    )\n",
    "    rf.fit(X_train, Y_train[:, i])\n",
    "    models_rf[t] = rf\n",
    "\n",
    "print(\"Trained RF models:\", list(models_rf.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056264a7",
   "metadata": {
    "papermill": {
     "duration": 0.004403,
     "end_time": "2026-01-06T19:08:17.026065",
     "exception": false,
     "start_time": "2026-01-06T19:08:17.021662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Inference: Extract features once per unique test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9717b110",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T19:08:17.036677Z",
     "iopub.status.busy": "2026-01-06T19:08:17.036309Z",
     "iopub.status.idle": "2026-01-06T19:08:17.771768Z",
     "shell.execute_reply": "2026-01-06T19:08:17.770800Z"
    },
    "papermill": {
     "duration": 0.74331,
     "end_time": "2026-01-06T19:08:17.773651",
     "exception": false,
     "start_time": "2026-01-06T19:08:17.030341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL4] extract_features: 1 images | batch_size=64 | workers=0\n",
      "[CELL4] Dataset init: 1 images\n",
      "[CELL4] Example path: /kaggle/input/csiro-biomass/test/ID1001187975.jpg\n",
      "[CELL4] total_batches=1\n",
      "[CELL4] First batch: (1, 3, 224, 224)\n",
      "[CELL4] Progress: 1/1\n",
      "[CELL4] features shape: (1, 512)\n",
      "X_test: (1, 512)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>target_name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test/ID1001187975.jpg</td>\n",
       "      <td>Dry_Clover_g</td>\n",
       "      <td>1.059608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test/ID1001187975.jpg</td>\n",
       "      <td>Dry_Dead_g</td>\n",
       "      <td>19.334812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test/ID1001187975.jpg</td>\n",
       "      <td>Dry_Green_g</td>\n",
       "      <td>27.271936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test/ID1001187975.jpg</td>\n",
       "      <td>Dry_Total_g</td>\n",
       "      <td>59.275982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test/ID1001187975.jpg</td>\n",
       "      <td>GDM_g</td>\n",
       "      <td>38.999027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              image_path   target_name     target\n",
       "0  test/ID1001187975.jpg  Dry_Clover_g   1.059608\n",
       "1  test/ID1001187975.jpg    Dry_Dead_g  19.334812\n",
       "2  test/ID1001187975.jpg   Dry_Green_g  27.271936\n",
       "3  test/ID1001187975.jpg   Dry_Total_g  59.275982\n",
       "4  test/ID1001187975.jpg         GDM_g  38.999027"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 7 — Inference (extract features once per unique test image)\n",
    "# ============================================\n",
    "needed_cols = {\"sample_id\", \"image_path\", \"target_name\"}\n",
    "if not needed_cols.issubset(test_df.columns):\n",
    "    raise ValueError(f\"test.csv must contain columns {sorted(needed_cols)}\")\n",
    "\n",
    "unique_test_paths = test_df[\"image_path\"].drop_duplicates().tolist()\n",
    "unique_test_abs = [os.path.join(DATA_DIR, p) for p in unique_test_paths]\n",
    "\n",
    "X_test = extract_features(unique_test_abs, batch_size=64)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "\n",
    "pred_matrix = np.zeros((len(unique_test_paths), len(TARGET_NAMES)), dtype=np.float32)\n",
    "for j, t in enumerate(TARGET_NAMES):\n",
    "    pred_matrix[:, j] = models_rf[t].predict(X_test)\n",
    "\n",
    "pred_wide = pd.DataFrame(pred_matrix, columns=TARGET_NAMES)\n",
    "pred_wide[\"image_path\"] = unique_test_paths\n",
    "\n",
    "pred_long = pred_wide.melt(\n",
    "    id_vars=\"image_path\",\n",
    "    var_name=\"target_name\",\n",
    "    value_name=\"target\"\n",
    ")\n",
    "\n",
    "pred_long.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a90bcf",
   "metadata": {
    "papermill": {
     "duration": 0.004501,
     "end_time": "2026-01-06T19:08:17.783787",
     "exception": false,
     "start_time": "2026-01-06T19:08:17.779286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b09b7981",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T19:08:17.795412Z",
     "iopub.status.busy": "2026-01-06T19:08:17.795040Z",
     "iopub.status.idle": "2026-01-06T19:08:17.823317Z",
     "shell.execute_reply": "2026-01-06T19:08:17.821455Z"
    },
    "papermill": {
     "duration": 0.037657,
     "end_time": "2026-01-06T19:08:17.826113",
     "exception": false,
     "start_time": "2026-01-06T19:08:17.788456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Validating pred_long...\n",
      "Step 2: Reading raw test file from /kaggle/input/csiro-biomass/test.csv...\n",
      "Step 3: Merging predictions onto raw test structure...\n",
      "Step 4: Finalizing output columns...\n",
      "[DEBUG] Missing target values (NaN) before fill: 0\n",
      "Step 5: Saving submission to /kaggle/working/submission.csv ...\n",
      "✅ Submission saved: /kaggle/working/submission.csv | shape=(5, 2)\n",
      "✅ Kaggle artifact check passed: /kaggle/working/submission.csv exists\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 8 — Submission Export (Kaggle-safe)\n",
    "# Ensures /kaggle/working/submission.csv ALWAYS exists\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "SUBMISSION_PATH = \"/kaggle/working/submission.csv\"\n",
    "\n",
    "def create_submission_file_from_pred_long(\n",
    "    pred_long: pd.DataFrame,\n",
    "    raw_test_path: str,\n",
    "    output_path: str = SUBMISSION_PATH\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates Kaggle submission in required long format:\n",
    "      - reads raw test.csv to get exact sample_id ordering/structure\n",
    "      - merges predictions on (image_path, target_name)\n",
    "      - outputs columns: [sample_id, target]\n",
    "      - saves to /kaggle/working/submission.csv\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Validating pred_long...\")\n",
    "    required_cols = {\"image_path\", \"target_name\", \"target\"}\n",
    "    if pred_long is None or not isinstance(pred_long, pd.DataFrame):\n",
    "        raise ValueError(\"pred_long is None or not a DataFrame.\")\n",
    "    if not required_cols.issubset(pred_long.columns):\n",
    "        raise ValueError(\n",
    "            f\"pred_long must contain columns {sorted(required_cols)}. \"\n",
    "            f\"Got: {list(pred_long.columns)}\"\n",
    "        )\n",
    "\n",
    "    print(f\"Step 2: Reading raw test file from {raw_test_path}...\")\n",
    "    raw_test = pd.read_csv(raw_test_path)\n",
    "\n",
    "    needed = {\"sample_id\", \"image_path\", \"target_name\"}\n",
    "    if not needed.issubset(raw_test.columns):\n",
    "        raise ValueError(\n",
    "            f\"test.csv must contain columns {sorted(needed)}. \"\n",
    "            f\"Got: {list(raw_test.columns)}\"\n",
    "        )\n",
    "\n",
    "    print(\"Step 3: Merging predictions onto raw test structure...\")\n",
    "    final_submission = pd.merge(\n",
    "        raw_test[[\"sample_id\", \"image_path\", \"target_name\"]],\n",
    "        pred_long[[\"image_path\", \"target_name\", \"target\"]],\n",
    "        on=[\"image_path\", \"target_name\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    print(\"Step 4: Finalizing output columns...\")\n",
    "    final_output = final_submission[[\"sample_id\", \"target\"]].copy()\n",
    "\n",
    "    # Safety: fill missing predictions with 0 (and report how many)\n",
    "    missing = final_output[\"target\"].isna().sum()\n",
    "    print(\"[DEBUG] Missing target values (NaN) before fill:\", int(missing))\n",
    "    final_output[\"target\"] = final_output[\"target\"].fillna(0).astype(float)\n",
    "\n",
    "    # IMPORTANT: create output directory and write the exact file name\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    print(f\"Step 5: Saving submission to {output_path} ...\")\n",
    "    final_output.to_csv(output_path, index=False)\n",
    "\n",
    "    # Hard check: Kaggle needs this file to exist in /kaggle/working\n",
    "    if not os.path.exists(output_path):\n",
    "        raise RuntimeError(f\"Submission file was not created at {output_path}\")\n",
    "\n",
    "    print(f\"✅ Submission saved: {output_path} | shape={final_output.shape}\")\n",
    "    return final_output\n",
    "\n",
    "\n",
    "def _write_dummy_submission(raw_test_path: str, output_path: str = SUBMISSION_PATH) -> pd.DataFrame:\n",
    "    \"\"\"Always writes a minimal valid submission.csv to avoid Kaggle 'missing output' errors.\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    sub = pd.read_csv(raw_test_path)[[\"sample_id\"]].copy()\n",
    "    sub[\"target\"] = 0.0\n",
    "    sub.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Dummy submission saved: {output_path} | shape={sub.shape}\")\n",
    "    return sub\n",
    "\n",
    "\n",
    "# --- EXECUTE (Kaggle-safe wrapper) ---\n",
    "# Key change: we ALWAYS ensure submission.csv exists even if upstream failed.\n",
    "try:\n",
    "    submission = create_submission_file_from_pred_long(\n",
    "        pred_long=pred_long,\n",
    "        raw_test_path=TEST_CSV,\n",
    "        output_path=SUBMISSION_PATH\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error generating submission from predictions: {e}\")\n",
    "    submission = _write_dummy_submission(TEST_CSV, SUBMISSION_PATH)\n",
    "\n",
    "# Final assert for Kaggle artifact detection\n",
    "assert os.path.exists(SUBMISSION_PATH), f\"submission.csv missing at {SUBMISSION_PATH}\"\n",
    "print(\"✅ Kaggle artifact check passed: /kaggle/working/submission.csv exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec06c9",
   "metadata": {
    "papermill": {
     "duration": 0.005276,
     "end_time": "2026-01-06T19:08:17.836575",
     "exception": false,
     "start_time": "2026-01-06T19:08:17.831299",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc337e",
   "metadata": {
    "papermill": {
     "duration": 0.004566,
     "end_time": "2026-01-06T19:08:17.845964",
     "exception": false,
     "start_time": "2026-01-06T19:08:17.841398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SIDE NOTES"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "datasetId": 2847,
     "sourceId": 4958,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 162.906607,
   "end_time": "2026-01-06T19:08:20.456911",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-06T19:05:37.550304",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
