{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## MARKDOWN TECHNIQUES FLOW","metadata":{}},{"cell_type":"code","source":"# OLD VERSION\n# import os, os.path \n\n# # --- KAGGLE CONFIGURATION ---\n\n# # 1. Define the Base Directory\n# # Kaggle inputs are always read-only and located in /kaggle/input\n# # You usually have a dataset folder name (e.g., 'biomass-data'). \n# # If you don't know the exact folder name yet, you can list directories:\n# # print(os.listdir(\"/kaggle/input\")) \n# BASE_DIR = '/kaggle/input' \n\n# # 2. Define Path to CSV\n# # Update 'your-dataset-name' to the actual folder name in Kaggle\n# CSV_PATH = os.path.join(BASE_DIR, 'your-dataset-name', 'train.csv')\n# TEST_CSV_PATH = os.path.join(BASE_DIR, 'your-dataset-name', 'test.csv')\n\n# # 3. Define Root Directory for Images\n# # Usually images are in a subfolder or directly in the dataset folder\n# IMAGE_ROOT = os.path.join(BASE_DIR, 'your-dataset-name', 'train')\n# TEST_IMAGE_ROOT = os.path.join(BASE_DIR, 'your-dataset-name', 'test')\n\n# # 4. Verification\n# if os.path.exists(CSV_PATH):\n#     print(f\"✅ Found CSV at: {CSV_PATH}\")\n# else:\n#     print(f\"❌ CSV not found at: {CSV_PATH}\")\n#     # Helper to find the real path if the above is wrong\n#     print(\"Available files in input:\", os.listdir(BASE_DIR))\n\n# if os.path.exists(IMAGE_ROOT):\n#     print(f\"✅ Found Image Dir at: {IMAGE_ROOT}\")\n# else:\n#     print(f\"❌ Image Dir not found at: {IMAGE_ROOT}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nKaggle to GitHub Sync Configuration\nRepository: CODES-ON-KAGGLE\n\"\"\"\n\nfrom pathlib import Path\nimport os\n\n# --- KAGGLE CONFIGURATION ---\n\n# 1. Define the Base Directory using pathlib\n# Kaggle inputs are read-only and located in /kaggle/input\nBASE_DIR = Path('/kaggle/input') \n\n# 2. Define the Dataset Name \n# Change 'your-dataset-name' to the folder name found in /kaggle/input\nDATASET_NAME = 'your-dataset-name'\nDATASET_PATH = BASE_DIR / DATASET_NAME\n\n# 3. Define Paths to CSVs and Image Directories\n# We use the / operator for clean, cross-platform path joining\nCSV_PATH = DATASET_PATH / 'train.csv'\nTEST_CSV_PATH = DATASET_PATH / 'test.csv'\n\nIMAGE_ROOT = DATASET_PATH / 'train'\nTEST_IMAGE_ROOT = DATASET_PATH / 'test'\n\n# 4. Verification Logic\ndef verify_paths():\n    paths_to_check = {\n        \"Training CSV\": CSV_PATH,\n        \"Testing CSV\": TEST_CSV_PATH,\n        \"Image Root\": IMAGE_ROOT\n    }\n    \n    print(f\"--- Path Verification for {DATASET_NAME} ---\")\n    for name, path in paths_to_check.items():\n        if path.exists():\n            print(f\"✅ {name} found at: {path}\")\n        else:\n            print(f\"❌ {name} NOT found at: {path}\")\n            \n    # If the main folder isn't found, list what is available\n    if not DATASET_PATH.exists():\n        print(f\"\\nAvailable directories in {BASE_DIR}:\")\n        print([p.name for p in BASE_DIR.iterdir() if p.is_dir()])\n\nif __name__ == \"__main__\":\n    verify_paths()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndef process_test_structure(csv_path):\n    \"\"\"\n    Parses the Kaggle-style test.csv into a format compatible with BiomassDataset.\n    Input: Raw test CSV with 5 rows per image.\n    Output: DataFrame with 1 row per image and 5 dummy target columns.\n    \"\"\"\n    # 1. Read the raw data\n    raw_df = pd.read_csv(csv_path)\n    \n    # 2. Extract unique images\n    # The CSV lists 5 rows for every 1 image (one for each target type).\n    # We slice with a stride of 5 to get unique image entries.\n    df_unique = raw_df.iloc[::5].copy()\n    \n    # 3. initialize the output DataFrame with the filename\n    # We reset the index to ensure it starts from 0 to len(df)-1\n    df_YY_new = df_unique[['image_path']].reset_index(drop=True)\n    \n    # 4. Add dummy target columns\n    # The Dataset class expects columns 1-5 to be float values. \n    # Since this is inference, we fill them with 0.0.\n    # The order MUST match the training set order.\n    target_cols = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n    \n    for col in target_cols:\n        df_YY_new[col] = 0.0\n        \n    return df_YY_new\n\n# Apply the function\ndf_YY = process_test_structure('test.csv')\n\n# Verify the structure\nprint(\"New df_YY shape:\", df_YY.shape)\nprint(df_YY.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}